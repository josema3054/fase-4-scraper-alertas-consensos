#!/usr/bin/env python3
"""
Script de diagn√≥stico urgente para identificar el problema espec√≠fico.
"""

import sys
import os
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime

# A√±adir src al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def urgent_diagnosis():
    """Diagn√≥stico urgente del problema de extracci√≥n"""
    
    print("=== DIAGN√ìSTICO URGENTE DEL SCRAPER ===")
    print(f"Fecha: {datetime.now()}")
    
    # URL exacta que usa el scraper
    today = datetime.now().strftime('%Y-%m-%d')
    url = f"https://contests.covers.com/consensus/topoverunderconsensus/mlb/expert/{today}"
    
    print(f"URL: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        print("\n1. Descargando contenido...")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        print(f"   ‚úÖ Descarga exitosa: {len(response.content)} bytes")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print("\n2. Buscando tablas...")
        tables = soup.find_all('table')
        print(f"   üìä Tablas encontradas: {len(tables)}")
        
        if not tables:
            print("   ‚ùå PROBLEMA: No hay tablas en la p√°gina")
            
            # Buscar divs que puedan contener datos
            print("\n   üîç Buscando contenido alternativo...")
            
            # Buscar elementos con texto de equipos MLB
            team_elements = soup.find_all(text=re.compile(r'[A-Z]{3}.*@.*[A-Z]{3}'))
            print(f"   Elementos con equipos: {len(team_elements)}")
            
            if team_elements:
                print("   üìã Ejemplos de equipos encontrados:")
                for i, elem in enumerate(team_elements[:3]):
                    print(f"      {i+1}. '{elem.strip()}'")
            
            # Buscar porcentajes
            percentage_elements = soup.find_all(text=re.compile(r'\d+%'))
            print(f"   Elementos con porcentajes: {len(percentage_elements)}")
            
            if percentage_elements:
                print("   üìã Ejemplos de porcentajes:")
                for i, elem in enumerate(percentage_elements[:5]):
                    print(f"      {i+1}. '{elem.strip()}'")
            
            return False
        
        print("\n3. Analizando tabla principal...")
        table = tables[0]  # Primera tabla
        
        rows = table.find_all('tr')
        print(f"   üìã Filas en tabla: {len(rows)}")
        
        print("\n4. Analizando filas...")
        
        valid_data_rows = 0
        for i, row in enumerate(rows[:10]):  # Primeras 10 filas
            cells = row.find_all('td')
            
            if len(cells) >= 3:  # Criterio del scraper mejorado
                row_text = row.get_text(strip=True)
                
                # Criterios de filtrado del scraper
                has_teams = bool(re.search(r'[A-Z]{2,3}', row_text))
                has_percentage = '%' in row_text
                has_time = bool(re.search(r'\d{1,2}:\d{2}', row_text))
                
                print(f"\n   Fila {i}: {len(cells)} celdas")
                print(f"   Texto: '{row_text[:80]}...'")
                print(f"   Criterios: Equipos={has_teams}, %={has_percentage}, Hora={has_time}")
                
                if has_teams and (has_percentage or has_time):
                    valid_data_rows += 1
                    print(f"   ‚úÖ V√ÅLIDA: Esta fila deber√≠a procesarse")
                    
                    # Mostrar contenido de celdas
                    for j, cell in enumerate(cells[:5]):
                        cell_text = cell.get_text(strip=True)
                        print(f"      Celda {j}: '{cell_text}'")
                else:
                    print(f"   ‚ùå NO V√ÅLIDA: No cumple criterios")
        
        print(f"\nüìä RESUMEN:")
        print(f"   Total filas: {len(rows)}")
        print(f"   Filas v√°lidas identificadas: {valid_data_rows}")
        
        if valid_data_rows == 0:
            print("   ‚ùå PROBLEMA: No hay filas que cumplan los criterios")
            print("   üîß SOLUCI√ìN: Necesitamos relajar m√°s los filtros")
        else:
            print("   ‚úÖ HAY DATOS V√ÅLIDOS: El problema est√° en la extracci√≥n")
            print("   üîß SOLUCI√ìN: Necesitamos revisar la l√≥gica de extracci√≥n")
        
        return valid_data_rows > 0
        
    except Exception as e:
        print(f"‚ùå Error durante diagn√≥stico: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = urgent_diagnosis()
    
    print(f"\n{'='*50}")
    if success:
        print("‚úÖ DIAGN√ìSTICO: Hay datos disponibles, problema en extracci√≥n")
    else:
        print("‚ùå DIAGN√ìSTICO: No hay datos disponibles o estructura cambi√≥")
    
    input("\nPresiona Enter para continuar...")
